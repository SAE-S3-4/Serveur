[![forthebadge](https://forthebadge.com/images/badges/made-with-java.svg)](https://forthebadge.com)![IntelliJ IDEA](https://img.shields.io/badge/IntelliJIDEA-000000.svg?style=for-the-badge&logo=intellij-idea&logoColor=white)</br>
# Prerequis
Le serveur as Ã©tÃ© structurÃ© en telle sorte quâ€™il puisse Ãªtre le plus flexible lors des modifications futures. En effet on pourrait lâ€™utiliser en installant juste le JRE de Java 17. 

## ParamÃ©trage
Pour modifier ses paramÃ©trages utiliser les diffÃ©rents arguments listÃ©es et expliquÃ©es en lanÃ§ant lâ€™application avec le paramÃ¨tre *--help*. 
- *-p* : pour modifier le port du serveur 
- *-maxConns* : pour modifier le numÃ©ro de connexions en simultanÃ©es au serveur admises 
- *-ai* : pour activer ou dÃ©sactiver lâ€™interaction dâ€™une IA dans lâ€™onglet conversation. 

Si vous lancez le serveur sans paramÃ¨tres on as Ã©galement configurÃ© des paramÃ¨tres par dÃ©faut comme : lâ€™utilisation du port 10013, lâ€™activation de lâ€™IA et 1000 connexions en simultanÃ©e. 

## Pour que le serveur soit fonctionnel il vous faudra installer docker. 

Il vous suffit de crÃ©er un script bash contenantâ€¯: 
```bash
#! /bin/bash 

docker stop serversaes4 

docker rm serversaes4 

docker rmi terminal 

docker run -d -v /var/run/docker.sock:/var/run/docker.sock -v /home/opc/llama.cpp:/usr/src/app/llama.cpp -p 10013:10013 --name serversaes4 fredegen/serversaes4:VOTRE ARCHITECTURE PROCESSEUR 
```

Ce script stoppe le serveur sâ€™il existe dÃ©jÃ , supprime le conteneur existant sâ€™il a dÃ©jÃ  Ã©tÃ© crÃ©Ã©, supprime lâ€™image Â«â€¯terminalâ€¯Â» qui est utilisÃ©e pour exÃ©cuter les commandes de chaque utilisateur et lance le serveur. La commande qui lance le serveur, le lance en mode Â«â€¯daemonâ€¯Â» câ€™est-Ã -dire en arriÃ¨re-plan. On lui donne accÃ¨s au volume /var/run/docker.sock pour quâ€™il puisse lancer des conteneurs docker. Le port 10013 de la machine hÃ´te est redirigÃ© vers le port 10013 du conteneur car câ€™est celui que nous utilisons pour les communications entre le serveur et le client. Il sera nommÃ© serversaes4 et sera crÃ©Ã© Ã  partir de lâ€™image fredegen/serversaes4â€¯:latest.  

Cette image est celle de notre serveur, si vous ne lâ€™avez pas tÃ©lÃ©chargÃ©e prÃ©cÃ©demment, elle sera tÃ©lÃ©chargÃ©e automatiquement. 

Il est nÃ©cessaire de connaitre votre architecture processeur pour la remplacer dans le script bash. Il est compatible pour les machines amd64 et arm64. Vous avez donc la possibilitÃ© de remplacer le tag par ces deux valeurs. 

## Mise en place de l'IA
Lâ€™intelligence artificielle ne sera pas installÃ©e et mise en place avec lâ€™image docker puisque cette configuration est trop lourde. 

Si vous souhaitez la mettre en place lâ€™IA vous pouvez suivre le mode dâ€™emploi donnÃ©e sur le GitHub du projet officiel : https://github.com/ggerganov/llama.cpp 

 

Et une fois que le projet as Ã©tÃ© configurÃ© il faudra crÃ©er un fichier talk.sh contenant ce simple script bash : 
```bash
#!/bin/bash  

./main -m ./models/7B/ggml-model-q4_0.bin --repeat_penalty 1.0 -p "$1" 2>outPut.txt 
```
Ce script sera utilisÃ© par le serveur pour interagir avec lâ€™IA en lui passant un input et en rÃ©cupÃ©rant lâ€™output. 

## Membres du projet ğŸ§‘â€ğŸ’»

Ceccarelli Luca</br>
Egenscheviller FrÃ©dÃ©ric</br>
Ramdani Djibril</br>
Saadi Nils</br>
Vial Amaury
